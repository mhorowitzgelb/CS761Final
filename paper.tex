\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{color}
\usepackage{parskip}

\setlength{\parskip}{5pt}
\setlength{\parindent}{0pt}

\title{Test}
\author{MAx}

\begin{document}

\maketitle

\section*{Modified Teaching environment}
We will solve a more relaxed problem such that the teacher may not only give $(x,y)$ training examples but may also give single $x$ values to probe the learner for output. 

\section*{Theorem 1}
If given a perceptron with known starting parameter $w_0$, desired parameter $w^*$, and unknown learning rate $\eta \in [\alpha , \beta]$ for given $\epsilon > 0$, we can guarantee $\eta \in [\alpha', \beta']$ such that $\beta' - \alpha' \leq \epsilon$ using $1 + \lceil \log_2(\frac{\beta - \alpha}{\epsilon})\rceil$ examples.=

\subsection*{Proof:}
Assume $w_0 \neq cw^*$ for arbitrary $c > 0$, then there exists an $(x_1, y_1)$ such that $\text{sign}(\langle w^*,x_1 \rangle) = y_1 = 1$ and $\text{sign}(\langle w_0,x_1 \rangle) = -y_1$.
\color{red}
I don't think that its hard to show this is true.
\color{black}

Then give $(x_1, y_1)$ to the learner and then after training update,
$$
w_1 = w_0 + \eta* x_1
$$

Then grab an unlabeled $x_2$ such that
$\langle w_0,x_2 \rangle \neq 0$, $\langle x_1, x_2 \rangle \neq 0$
and 
$$
\langle w_0,x_2 \rangle + \frac{\beta - \alpha}{2} \langle x_1, x_2 \rangle = 0
$$
\color{red}
I'm a moron. What's the closed form of this? I know there is not a unique solution. We just need the simplest one.
\color{black}

Then probe the learner with $x_2$. If the learner outputs $+1$ then
$\eta \in (\frac{\beta - \alpha}{2} , \beta]$, else 
$\eta \in [\alpha,\frac{\beta - \alpha}{2}]$

As you can see our interval for $\eta$ halves with each probe. One can then see that we can iteratively repeat this for a total of $ \lceil \log_2(\frac{\beta - \alpha}{\epsilon})\rceil$ iterations to aquire the desired interval length on $\eta$.


\section*{Theorem 2}
\color{red}

Something to the respect of, if we know $\eta \in [\alpha, \beta]$ then with one training example we can guarantee that
$$
\frac{\langle w^*, w_0 \rangle}{\lvert w^* \rvert \lvert w_0 \rvert} \geq 1 - \epsilon
$$

So by combining with Theorem 1
$$
T_\epsilon \leq \{\text{number of points needed to guarantee } \eta \in [\alpha, \beta]\} +1 
$$
Where $T_\epsilon$ is the $\epsilon$ teaching dimension of the perceptron.
\color{black}

\subsection*{Proof}


Assume $w_0$ is known and $\eta \in [\eta_{\min} , \eta_{\max}] $ such that 
$0 < \eta_{\min} < \eta_{\max} < \infty$

Assume WLOG $\lVert w^* \rVert = 1$.

Pick $(x_1,y_1)$ such that $y_1 = -1$, $\lVert x_1 \rVert = \frac{2 \lVert w_0 \rVert}{\epsilon \eta_{\min}}$

\color{red}
Note when i say $y_1 = 1$, that implies that it agrees with $w^*$. And I'm pretty 
sure the above statement is valid. I think such an $x,y$ pair will always exist if
$w_0$ disagrees with $w^*$.
\color{black}

Then use the method from Theorem 1 to get a bound on $\eta$ such that $\eta \in [\alpha , \beta]$ and $\beta - \alpha <= 2\epsilon \eta_{\max}$

Then let $x_2 = \frac{2 \lVert w_0 \rVert}{\epsilon (\beta + \alpha)/2}w^* + x_1$.
It should be clear that then $y_2 = 1$

Then realize that
$w_1 = w_0 - \eta x_1$

So
$$
y_2 \langle w_1, x_2 \rangle  = \bigg\langle \frac{2 \lVert w_0 \rVert}{\epsilon (\beta + \alpha) /2} w^* + x_1 , w_0 - \eta x_1 \bigg\rangle
$$
$$
= \bigg\langle \frac{2 \lVert w_0 \rVert}{\epsilon (\beta + \alpha) /2} w^* + x_1 , w_0  \bigg\rangle - \eta \lVert x_1 \rVert^2
$$
$$
\leq \bigg( \frac{2 \lVert w_0 \rVert}{\epsilon (\beta + \alpha) /2} \lVert w^* \rVert + \lVert x_1 \rVert \bigg ) \lVert w_0 \rVert - \eta \lVert x_1 \rVert^2
$$
$$
= \frac{2 \lVert w_0 \rVert^2}{\epsilon (\beta + \alpha)/2} + \frac{2 \lVert w_0 \rVert ^2}{\epsilon \eta_{\min}} - \eta \frac{4 \lVert w_0 \rVert^2}{\epsilon^2 \eta_{\min}^2}
$$
$$
\leq \frac{4 \lVert w_0 \rVert^2}{\epsilon (\beta + \alpha)/2}  -  \frac{4 \lVert w_0 \rVert^2}{\epsilon^2 \eta_{\min}}
$$
which is less than $0$ for $\epsilon < 1$

Then,
notice that $w_2 = w_0 +\eta \frac{2  \lVert w_0 \rVert}{\epsilon (\beta + \alpha)/2} w^*$

Then 
$$
\frac{\langle w_2, w^* \rangle}{\lVert w_1 \rVert \lVert w^* \rVert} = \frac{
\frac{\eta 2  \lVert w_0 \rVert}{\epsilon (\beta + \alpha)/2} \lVert w^* \lVert ^2 + \langle w_0, w^* \rangle}{\lVert w_2 \rVert}
$$ 
$$
\geq \frac{\frac{\eta 2  \lVert w_0 \rVert}{\epsilon (\beta + \alpha)/2} - \lVert w_0 \rVert}{\lVert w_0 \rVert +\frac{\eta 2  \lVert w_0 \rVert}{\epsilon (\beta + \alpha)/2} }
$$
$$
= 1 - \frac{2 \lVert w_0  \rVert}{\lVert w_0 \rVert +\frac{\eta 2  \lVert w_0 \rVert}{\epsilon (\beta + \alpha)/2} }
$$
$$
= 1 - \frac{2\lVert w_0 \rVert}{\lVert w_0  \rVert \epsilon + \frac{\eta 2  \lVert w_0 \rVert}{(\beta + \alpha)/2} } \epsilon
$$
$$
\geq  1 - \frac{2\lVert w_0 \rVert}{\lVert w_0  \rVert \epsilon + \frac{\eta 2  \lVert w_0 \rVert}{\eta_{\max} - (\beta - \alpha) / 2} } \epsilon
$$
$$
\geq  1 - \frac{2\lVert w_0 \rVert}{\lVert w_0  \rVert \epsilon + \frac{\eta 2  \lVert w_0 \rVert}{\eta_{\max} (1-\epsilon)} } \epsilon
$$
$$
\geq 1 - \frac{2\lVert w_0 \rVert}{\lVert w_0  \rVert \epsilon + \frac{ 2  \lVert w_0 \rVert}{(1-\epsilon)} } \epsilon
$$
$$
\geq 1 - \epsilon
$$

Therefore 
$T_\epsilon <=  2 + \lceil \log_2 \frac{\eta_{\max} - \eta_{\min}}{2\epsilon\eta_{\max}}  \rceil$




\end{document}