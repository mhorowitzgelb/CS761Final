\documentclass{article}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{color}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{bbm}

\setlength{\parskip}{5pt}
\setlength{\parindent}{0pt}

\title{\rule{350pt}{5pt}\\ \textbf{An Extension of Optimal Teaching For Online Perceptrons} \\ \rule{350pt}{1pt}}


\author{
  \textbf{Max Horowitz-Gelb \qquad Gabriel Ferns}\\
  Department of Computer Sciences, University of Wisconsin-Madison\\
  \texttt{\{horowitzgelb, ferns\}@wisc.edu}\\
}

\newcommand{\learn}{\mathcal{A}_{w_0}^\eta}
\newcommand{\radian}{\text{radian}}


\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
  Optimal teaching is the task of constructing an efficient training set
  that teaches a learner some desired concept inside its concept class. Before
  \cite{perceptron}, analysis of optimal teaching was only focused on batch
  learners. \cite{perceptron} looked at the optimal teaching strategy of an
  online Perceptron with varying degrees of knowledge of the initial state of
  the learner. In this paper, we extend these results to environments where
  there is even less knowledge of the Perceptron's initial state, but the
  teacher is given extra powers to compensate. We present an efficient algorithm
  to force the learner into a desired state with no knowledge of the learner's
  initial state. We also analyze a setting in which a teacher has the ability to
  lie.
\end{abstract}

\section{Introduction}
====
Mine

In the real world, there are often scenarios where a teacher wishes to
embed a desired concept within a learner. Sometimes, the teacher can only
interact with the learner by providing the learner with a set of training examples.
The task of creating the smallest possible set of examples that
produces the desired concept in the learner is known as optimal teaching
\cite{machine_teaching} \cite{teaching_dimension}. \cite{perceptron} looked for
the first time at optimal teaching in an online environment by proving the
optimal teaching dimension in an online Perceptron. In this paper we look at the
optimal teaching of the same perceptron, but with a teacher that also possess
the ability to use unlabeled examples. An unlabeled example gives the teacher
knowledge of the learner's prediction about that example without altering the
learner's state. This is a reasonable power for any real-world teacher to
posses. We show that this added ability allows the teacher to teach an
online perceptron in a zero-knowledge scenario that otherwise would have been
difficult.

The specific perceptron we are teaching is the online perceptron
designed for binary classification \cite{perceptron_algo}. It can learn any
linear decision boundary with mapping $x \to sign(\langle w, x \rangle)$, where
$x \in \mathbb{R}^d$ is in the input space and $w \in \mathbb{R}^d$
parameterizes the perceptron. Here, as in \cite{perceptron}, $sign(0) = -1$ for
completeness. The learning algorithm for the online perceptron is seen below.

====
Yours

There may be a scenario where one entity, or the \textit{teacher},  has a desired target concept and a learner in which the teacher wants to embed that concept. In this scenario the teacher can only interact with the learner by constructing and then giving the learner a set of training examples. This constructive task is known as optimal teaching \cite{machine_teaching} \cite{teaching_dimension}. It has applications in education and security \cite{security}\cite{education} \cite{poisoning}. \cite{perceptron}\cite{td_linear} looked for the first time at online teaching in an online environment, with an online perceptron. In this paper we look at optimal teaching of the same perceptron, but now we give the teacher the ability to use unlabeled examples. Unlabeled examples allow the teacher to probe to discover more about the learners current state while not changing the learner. We show that this new added ability allows the teacher to teach an online perceptron in a scenario, that before may have been difficult.

The specific perceptron we are teaching is the  online perceptron \cite{perceptron_algo}. The algorithm is designed for a binary classification. It can learn any linear decision boundary,
with mapping $x \to sign(\langle w, x \rangle)$ where $x \in \mathbb{R}^d$ comes from the input space and $w \in \mathbb{R}^d$ parameterizes the perceptron. Here , as in \cite{perceptron}, $sign(0) = -1$ for completeness. The learning algorithm for the online perceptron is seen below.

====

\begin{algorithm}
\caption{Online Perceptron}
\begin{algorithmic}[1]
	\REQUIRE Initial weight vector $w_0 \in \mathbb{R}^d$, learning rate $\eta > 0$
	\FOR{$t \in 1$ ...}
    	\STATE receive $x_t$
    	\STATE predict $\hat{y}_t = sign(\langle w_{t-1} , x_t \rangle)$
    	\STATE receive $y_t$
    	\STATE $w_t \leftarrow w_{t-1} + \mathbbm{1}_{y_t \langle w_{t-1} , x_t \rangle \leq 0} \eta x_t$
    \ENDFOR
\end{algorithmic}
\end{algorithm}

Because $w_0$ and $\eta$ affect the Perceptron's learning, knowledge of them is
critical to being able to teach an online Perceptron learner. \cite{perceptron}
looked at teaching a perceptron with, and without knowledge of $w_0$. Here we
look at the situation where $w_0$ is known, but the learning rate $\eta$ is
unknown. We also look at the most challenging case, when both $w_0$ and $\eta$
are unknown at the start of teaching. 


\section{Teaching Dimension}
\cite{perceptron} used two metrics when analyzing their teaching strategies. The first was the exact 
teaching dimension: the length of the shortest training sequence that teaches a learner the desired concept. 

\subsection{$TD_{exact}$}

Formally, let $\mathbb{S}$ be the space of finite training 
sequences. Then let $\mathcal{A}_{w_0}^{\eta} : \mathbb{S} \to \mathbb{R}^d$ be a function the gives the 
resultant weight vector after training on a sequence in $\mathbb{S}$, where $w_0$ is the initial weight 
vector, and $\eta$ is the learning rate. Then if the teacher which to teach a
concept $w^*$, since $w^*$ describes a linear decision boundary for any $c > 0$,
$cw^*$ is parameterizes the same concept as $w^*$. So the exact teaching dimension is 
$$
TD_{exact}(w^*, \mathcal{A}_{w_0}^\eta) = \min\{ \lvert S \rvert : 
\mathcal{A}_{w_0}^\eta(S) = cw^* \text{ for some } c >0 \}
$$

\cite{perceptron} showed that when $\eta$ and $w_0$ where known, $TD_{exact}(w^*, \mathcal{A}_{w_0}^\eta) = 1$

\subsection{$TD_\epsilon$}
\cite{perceptron} also defined an approximate teaching dimension for cases where
the exact teaching dimension was infinite, but one could approximately teach a concept within $\epsilon$ distance. For perceptrons, the distance metric used was simply the 1 minus the cosine angle between $w^*$ and $\learn(S)$
$$
1 -\frac{\langle \learn(S), w^* \rangle}{\lVert \learn(s) \rVert \lVert w^* \rVert} \leq \epsilon
$$

Then let $\mathbb{T}_\epsilon$ be the set of $\epsilon$-approximate teaching strategies for a decision boundary $w^*$. Then
$$
TD_\epsilon(w^*, \learn) = \min_{t \in 
\mathbb{T}_\epsilon} \max_{w_0 \in 
\mathbb{R}^d} \lvert t(w_0) \rvert
$$

\cite{perceptron} showed that when $w_0$ was unknown but $\eta$ was known, $TD_\epsilon(w^*, \learn) = 3$

\section{Approximate Teaching with Unkown $\eta$}
Now we look at the scenario where the initial weight vector is known but the learning rate is unknown and bounded.

\subsection{Using Unlabeled Examples}
Before, the teaching dimension was only analyzed with labeled examples. Here we relax this constraint and allow unlabeled examples to be used in the teaching sequence. Now we redefine $\mathbb{S}$ to be the set of all sequences of labeled and unlabeled examples. And for any $S \in \mathbb{S}$
$$
\learn(S) = \learn(S \setminus \{s \in S : s = (x, ?)\} )
$$

\subsection{Algorithm for bounding $\eta$}
With the ability to teach with unlabeled examples, below shows the optimal algorithm for bounding $\eta$ within a desired length, under the conditions that $\eta$ is bounded and $w_0$ is known a priori.

\begin{algorithm}
\caption{Bounding $\eta$ within $\epsilon$ }
\begin{algorithmic}[1]
	\REQUIRE $\epsilon$ , lower bound $\eta_{\min} > 0$, upper bound $\eta_{\max} < \infty$ and $w^*$
	\STATE Select $(x_1 , y_1)$ s.t. $sign(\langle w^* , x_1 \rangle) = y_1 = -1 = 
	-sign(\langle w_0 , x_1 \rangle )$
	\STATE Give $(x_1, y_1)$ to the perceptron and allow to update.
	\STATE $\alpha \leftarrow \eta_{\min}$
	\STATE $\beta \leftarrow \eta_{\max}$
	
	\WHILE{$\beta - \alpha > \epsilon$}
		\STATE Select $ x_2$ s.t. $\langle w_0 , x_2 \rangle \neq 0, \langle x_1 , x_2 \rangle \neq 0$ and $\langle w_0, x_2 \rangle - \frac{\beta - \alpha}{2}\langle x_1, x_2 \rangle = 0$
		\STATE Let the perceptron predict $\hat{y}$ for $x_2$
		\IF{$\hat{y} = 1$}
			\STATE $ \beta \leftarrow \frac{\beta - \alpha}{2}$ 		
		\ELSE
			\STATE $ \alpha \leftarrow \frac{\beta - \alpha}{2}$ 
		\ENDIF	
	\ENDWHILE   
	\RETURN $(\alpha , \beta)$
\end{algorithmic}
\end{algorithm} 

\textbf{Theorem 1.}
\textit{
If given a perceptron with known starting parameter $w_0$, desired parameter $w^*$, and unknown 
learning rate $\eta \in [\eta_{\min} , \eta_{\max}]$, then for given $\epsilon > 0$, \textbf{Algorithm 2} can guarantee $\eta \in 
[\alpha, \beta]$ such that $\beta - \alpha \leq \epsilon$ using $1 + \lceil \log_2(\frac{\eta_
{\min} - \eta_{\max}}{\epsilon})\rceil$ examples.}

\textit{Proof.}
Assume $w_0 \neq cw^*$ for arbitrary $c > 0$, then there exists an $(x_1, y_1)$ such that $\text{sign}(\langle w^*,x_1 \rangle) = y_1 = -1$ and $\text{sign}(\langle w_0,x_1 \rangle) = -y_1$.
\color{red}
I don't think that its hard to show this is true. But I won't prove for now
\color{black}

Then give $(x_1, y_1)$ to the learner and then after training update,
$$
w_1 = w_0 + \eta x_1
$$

Then grab an unlabeled $x_2$ such that
$\langle w_0,x_2 \rangle \neq 0$, $\langle x_1, x_2 \rangle \neq 0$
and 
$$
\langle w_0,x_2 \rangle - \frac{\eta_{\max} - \eta_{\min}}{2} \langle x_1, x_2 \rangle = 0
$$
\color{red}
There should always be at least one solution to this. Again I won't prove for now.
\color{black}


Then probe the learner with $x_2$. If the learner outputs $-1$ then
$\eta \in (\frac{\eta_{\max} - \eta_{\min}}{2} , \eta_{\max}]$, else 
$\eta \in [\eta_{\min},\frac{\eta_{\max} - \eta_{\min}}{2}]$

After the first labeled example , the length of the bound on $\eta$ is halved which each unlabeled example given. So one can see that with $n + 1$ examples the length of the bound on $\eta$ is $\frac{\eta_{\max} - \eta_{\min}}{2 ^ n }$. So to achieve a bound length of $\epsilon$ one needs
$ 1 + \lceil \log_2 ( \frac{\eta_
{\min} - \eta_{\max}}{\epsilon} ) \rceil$ examples.

\subsection{Teaching Dimension for Unknown $\eta$}

\textbf{Theorem 2.}
\textit{ If $w_0$ is known , $w_0 \leq b < \infty$ and $ 0 < \eta_{\min} \leq \eta \leq \eta_{\max} < \infty$ 
then $\mathbb{T}_{\epsilon}(w^*, \learn)  \leq 2 + \lceil \log_2 \frac{\eta_{\max} - \eta_{\min}}{2\epsilon\eta_{\max}}  \rceil$
}
\\
\\
\textit{Proof.}
Assume WLOG $\lVert w^* \rVert = 1$.

Pick $(x_1,y_1)$ such that $y_1 = -1$, $\lVert x_1 \rVert = \frac{2 b}{\epsilon \eta_{\min}}$ and $\langle x_1 , w^* \rangle = 0$.

Use this $(x_1, y_1)$ in \textbf{Algorithm 2} to get a bound on $\eta$ such that $\eta \in [\alpha , \beta]$ and $\beta - \alpha <= 2\epsilon \eta_{\max}$

Then let $x_2 = \frac{2 b}{\epsilon (\beta + \alpha)/2}w^* + x_1$.
It should be clear that then that $sign(\langle w* , x_2 \rangle) =  y_2 = 1$

Then realize that
$w_1 = w_0 - \eta x_1$

So
$$
y_2 \langle w_1, x_2 \rangle  = \bigg\langle \frac{2 b}{\epsilon (\beta + \alpha) /2} w^* + x_1 , w_0 - \eta x_1 \bigg\rangle
$$
$$
= \bigg\langle \frac{2 b}{\epsilon (\beta + \alpha) /2} w^* + x_1 , w_0  \bigg\rangle - \eta \lVert x_1 \rVert^2
$$
$$
\leq \bigg( \frac{2 b}{\epsilon (\beta + \alpha) /2} \lVert w^* \rVert + \lVert x_1 \rVert \bigg ) b - \eta \lVert x_1 \rVert^2
$$
$$
\leq \frac{4 b ^2}{\epsilon \eta_{\min}} - \eta \frac{4 b^2}{\epsilon^2 \eta_{\min}^2}
$$
$$
\leq \frac{4 b^2}{\epsilon \eta_{\min}}  -  \frac{4 b^2}{\epsilon^2 \eta_{\min}}
$$
which is less than $0$ for $\epsilon < 1$, so $(x_2 , y_2)$ will cause the perceptron to update.

Then,
notice that $w_2 = w_0 +\eta \frac{2  b}{\epsilon (\beta + \alpha)/2} w^*$

Then 
$$
\frac{\langle w_2, w^* \rangle}{\lVert w_2 \rVert \lVert w^* \rVert} = \frac{
\frac{\eta 2  b}{\epsilon (\beta + \alpha)/2} \lVert w^* \lVert ^2 + \langle w_0, w^* \rangle}{\lVert w_2 \rVert}
$$ 
$$
\geq \frac{\frac{\eta 2  b}{\epsilon (\beta + \alpha)/2} - b}{b +\frac{\eta 2  \lVert w_0 \rVert}{\epsilon (\beta + \alpha)/2} }
$$
$$
= 1 - \frac{2 b}{b +\frac{\eta 2  b}{\epsilon (\beta + \alpha)/2} }
$$
$$
= 1 - \frac{2b}{b \epsilon + \frac{\eta 2  b}{(\beta + \alpha)/2} } \epsilon
$$
$$
\geq  1 - \frac{2b}{b \epsilon + \frac{\eta 2  b}{\eta_{\max} - (\beta - \alpha) / 2} } \epsilon
$$
$$
\geq  1 - \frac{2b}{b \epsilon + \frac{\eta 2  b}{\eta_{\max} (1-\epsilon)} } \epsilon
$$
$$
\geq 1 - \frac{2b}{b \epsilon + \frac{ 2  b}{(1-\epsilon)} } \epsilon
$$
$$
\geq 1 - \epsilon
$$

Therefore 
$\mathbb{T}_\epsilon \leq  2 + \lceil \log_2 \frac{\eta_{\max} - \eta_{\min}}{2\epsilon\eta_{\max}}  \rceil$

\section{Teaching Dimension for unknown $\eta$ and $w_0$}
Here we present an algorithm that teaches the desired concept to the Perceptron learner when we know neither the learning rate, nor the initial weight vector of
our perceptron learner, in ? steps.

\subsection{$\epsilon$-Bounding the angle of $w_0$}
Our algorithm will only consider the decision boundary and the weight vector of
the Perceptron in two dimensions. In other words, we are considering the plane
defined by
\[
  \begin{bmatrix}
    a \\
    b \\
    0 \\
    $\vdots$ \\
    0
  \end{bmatrix}
  \textnormal{where}
  \ a, b \in \textbb{R}
\]
\\
\\
In order to proceed, we must define an operation that creates an
$\epsilon$-Bound around the angle of $w_o$ in the plane. It works by iteratively
partitioning the plane with unlabeled examples. For convenience, let $(a,b)$
denote a point in said plane. Let $ang(x,y)$ denote the angle between $x$ and
$y$.

\algnewcommand\algorithmicswitch{\textbf{switch}}
\algnewcommand\algorithmiccase{\textbf{case}}

\begin{algorithm}
\caption{Bounding the angle of $w_0$}
\begin{algorithmic}[1]
	\REQUIRE $\epsilon$

  \SWITCH{$sign(\langle w_0 , (0,1) \rangle)$, $sign(\langle w_0 , (1,0) \rangle)$}
  \CASE
  
  \ENDCASE
  \ENDSWITCH
  \IF{$\alpha \leftarrow (1,0)$}  \ENDIF 
  
	\STATE Select $(x_1 , y_1)$ s.t. $sign(\langle w^* , x_1 \rangle) = y_1 = -1 = 
	-sign(\langle w_0 , x_1 \rangle )$
	\STATE Give $(x_1, y_1)$ to the perceptron and allow to update.
	\STATE $\alpha \leftarrow \eta_{\min}$
	\STATE $\beta \leftarrow \eta_{\max}$
	
	\WHILE{$\beta - \alpha > \epsilon$}
		\STATE Select $ x_2$ s.t. $\langle w_0 , x_2 \rangle \neq 0, \langle x_1 , x_2 \rangle \neq 0$ and $\langle w_0, x_2 \rangle - \frac{\beta - \alpha}{2}\langle x_1, x_2 \rangle = 0$
		\STATE Let the perceptron predict $\hat{y}$ for $x_2$
		\IF{$\hat{y} = 1$}
			\STATE $ \beta \leftarrow \frac{\beta - \alpha}{2}$ 		
		\ELSE
			\STATE $ \alpha \leftarrow \frac{\beta - \alpha}{2}$ 
		\ENDIF	
	\ENDWHILE   
	\RETURN $(\alpha , \beta)$
\end{algorithmic}
\end{algorithm} 
\textbf{textbf}

\subsection{$\epsilon$-Bounding $\eta$}

Assume $\exists b, \eta_{min}, \eta_{max} \in \mathbb{R}$ such that $\lVert w_0
\rVert \leq b < \infty$ and $0 < \eta_{\min} \leq \eta \leq \eta_{\max} <
\infty$. 



Now that we have a bound around $\eta$, we get to the main result.
\\
\\
\textbf{Theorem 4.} 
\textit{With unknown $w_0$ such that $\lVert w_0 \rVert \leq b < \infty$
and unknown $\eta$ such that $0 < \eta_{\min} \leq \eta \leq \eta_{\max} < \infty$ then 
$
\mathbb{T}_\epsilon(w^* , \learn) \leq 3 + ?
$
}
\\
\\
\textit{Proof.}
First using \textbf{Algorithm 3} bound $\eta$ such that $ \alpha \leq \eta \leq \beta$ and $\beta - \alpha \leq 2\epsilon \eta_{\max}$

Then WLOG assume $\lVert w^8 \rVert = 1$. Then pick an $x_1$ such that 
$\lVert x_1 \rVert = \frac{2b}{\epsilon (\beta + \alpha)/2}$ and $\langle w^*, x_1 \rangle = 0$. Such an $x_1$ always exists. Then, let $x'_1 = -x_1.$ Then $y_1 = y'_1 = \text{sign}(\langle w*, x_1 \rangle) = -1$.  Then since $y_1 = y'_1$ then the perceptron must make an error on one of the examples. So first give $(x_1, y_1)$ to the perceptron.
\underline{Case 1}: If $(x_1, y_1)$ then $w_1 = w_0 - \eta x_1$. 
Then select at the second iteration an $x_2 = \frac{2b}{\epsilon (\beta + \alpha)/2}$. Using the same logic from theorem 2, we can see that this will give us our desired $\epsilon$ approximate $w_2$. 

\underline{Case 2}: If $(x_1, y_1)$ did not cause and update then set $x_2 = x'_1$ , $y_2 = y'_1$ and $x_3 = \frac{2b}{\epsilon (\beta + \alpha)/2}w^* + x_2$. Using very similar logic to how Case 1 used Theorem 2, we can see that this will also get us the desired $\epsilon$ approximate $w_3$.

In Case 1 we require 2 examples and and Case 2 we require 3, so including the examples needed to bound $\eta$ we get that 
$$
\mathbb{T}_\epsilon(w^*, \learn) \leq 3 + ?
$$

\subsection{Ability to Lie}
Allowing the teacher to lie, or give $(x,y)$ that disagree with $w^*$, make teaching the learner very easy. In fact in this scenario, the approximate teaching dimension is $2$.
\\
\\
\textbf{Theorem 5.} \textit{
Let $w_0$, $\eta$ be unknown with $\lVert w_0 \rVert \leq b$ and $0 < \eta_{\min} \leq \eta$.
Then with the ability to lie, $\mathbb{T}_{\epsilon}(w^*, \learn) = 2$
}
\\
\\
\textit{Proof.}
WLOG assume $\lVert w^* \rVert = 1$
Select an $x_1 = \frac{b(2-\epsilon)}{\eta_{\min} \epsilon}w^*, \qquad y_1 = -1$
Then give $(x_1 , y _1)$ to the perceptron.
Then let $x_2 = 2x_1, \qquad y_2 = 1$ and give $(x_2, y_2)$ to the perceptron.

Then there are two cases.

\textbf{Case 1:} The perceptron incorrectly predicts $y = 1$ for $x_1$.

Then 
$
w_1 = w_0 - \frac{\eta b(2-\epsilon)}{\eta_{\min} \epsilon}w^*
$

Then 
$$y_2 \langle w_1, 2x_1 \rangle
$$
$$
 = \langle w_0, 2x_1 \rangle - 2\frac{(\eta b(2-\epsilon))^2}{(\eta_{\min}\epsilon)^2}
 \leq 2b\frac{\eta b(2-\epsilon)}{\eta_{\min}\epsilon}
-  2\frac{(\eta b(2-\epsilon))^2}{(\eta_{\min}\epsilon)^2} 
$$
$$
\leq 2\frac{\eta b^2(2-\epsilon)}{\eta_{\min}\epsilon}
-  2\frac{\eta (b(2-\epsilon))^2}{\eta_{\min}\epsilon^2}
< 0 \qquad \text{ for } \epsilon < 0
$$
Therefore $(x_2, y_2)$ causes and update and
$w_2 =  w_0 + \frac{\eta b(2-\epsilon)}{\eta_{\min} \epsilon}w^* $

Then
$$
\frac{\langle w_2 , w^* \rangle}{\lVert w_2 \rVert \lVert w^* \rVert} = \frac{\frac{\eta b(2-\epsilon)}{\eta_{\min} \epsilon} + \langle w_0, w^* \rangle}{\lVert w_2 \rVert} \geq \frac{\frac{\eta b(2-\epsilon)}{\eta_{\min} \epsilon} - b}{b + \frac{\eta b(2-\epsilon)}{\eta_{\min} \epsilon}}
$$
Which implies
$$
1 - \frac{\langle w_2 , w^* \rangle}{\lVert w_2 \rVert \lVert w^* \rVert} \leq \frac{2b}{b + \frac{\eta b(2-\epsilon)}{\eta_{\min} \epsilon}}
\leq
\frac{2}{1 + \frac{2-\epsilon}{\epsilon}} = \epsilon
$$

\textbf{Case 2:} $(x_1, y_1)$ does not cause an update.

Then clearly $(x_2, y_2)$ will cause an update and
$$
w_2 = w_0 + 2\frac{\eta b(2-\epsilon)}{\eta_{\min} \epsilon}w^* 
$$
so,
$$
\frac{\langle w_2 , w^* \rangle}{\lVert w_2 \rVert \lVert w^* \rVert} = 2\frac{\frac{\eta b(2-\epsilon)}{\eta_{\min} \epsilon} + \langle w_0, w^* \rangle}{\lVert w_2 \rVert} \geq \frac{\frac{\eta b(2-\epsilon)}{\eta_{\min} \epsilon} + \langle w_0, w^* \rangle}{\lVert w_2 \rVert} \geq 1 - \epsilon
$$

We have thus shown that we can get an $\epsilon$ approximate bound with only 2 examples. It should be clear that this is impossible to do with $1$ example since without knowledge of $w_0$ there exists no $(x_1, y_1)$ that can guarantee the perceptron will update.

So 
$
\mathbb{T}_{\epsilon}(w^*, \learn) = 2
$

\textbf{Corrolary}
\textit{
Theorem 5 also shows that with known $\eta$ and ability to lie the approximate teaching dimension is still 2. This is 1 less the the $\epsilon$ approximate Teaching dimension where we cannot lie. \cite{perceptron}
}


\small
\bibliography{paper_bib}
\bibliographystyle{plain}


\end{document}
