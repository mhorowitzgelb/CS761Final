\documentclass{article}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{bbm}
\usepackage{parskip}

\setlength{\parskip}{5pt}
\setlength{\parindent}{0pt}

\title{\rule{350pt}{5pt}\\ \textbf{An Extension of Optimal Teaching For Online Perceptrons} \\ \rule{350pt}{1pt}}


\author{
  \textbf{Max Horowitz-Gelb \qquad Gabriel Ferns}\\
  Department of Computer Sciences, University of Wisconsin-Madison\\
  \texttt{\{horowitzgelb, ferns\}@wisc.edu}\\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
  Optimal teaching is the task of constructing a special efficient training set, which when given to a learner teaches the desired concept inside the learners concept class. Before \cite{perceptron}, analysis of optimal teaching was only focused on batch learners. \cite{perceptron} looked at the optimal teaching strategy of an online Perceptron in two different scenarios with varying degrees of knowledge of the initial state of the learner. In this paper we show extensions of these scenarios where knowledge of the initial state is even more unknown.
\end{abstract}

\section{Introduction}
There may be a scenario where one entity, or the \textit{teacher},  has a desired target concept and a learner in which the teacher wants to embed that concept. In this scenario the teacher can only interact with the learner by constructing and then giving the learner a set of training examples. This constructive task is known as optimal teaching \cite{machine_teaching} \cite{teaching_dimension}. \cite{perceptron} looked for the first time at online teaching in an online environment, with an online perceptron. In this paper we look at optimal teaching of the same perceptron, but now we give the teacher the ability to use unlabeled examples. Unlabeled examples allow the teacher to probe to discover more about the learners current state while not changing the learner. We show that this new added ability allows the teacher to teach an online perceptron in a scenario, that before may have been difficult.

The specific perceptron we are teaching is the  online perceptron \cite{perceptron_algo}. The algorithm is designed for a binary classification. It can learn any linear decision boundary,
with mapping $x \to sign(\langle w, x \rangle)$ where $x \in \mathbb{R}^d$ comes from the input space and $w \in \mathbb{R}^d$ parameterizes the perceptron. Here , as in \cite{perceptron}, $sign(0) = -1$ for completeness. The learning algorithm for the online perceptron is seen below.

\begin{algorithm}
\caption{Online Perceptron}
\begin{algorithmic}[1]
	\REQUIRE Initial weight vector $w_0 \in \mathbb{R}^d$, learning rate $\eta > 0$
	\FOR{$t \in 1$ ...}
    	\STATE receive $x_t$
    	\STATE predict $\hat{y}_t = sign(\langle w_{t-1} , x_t \rangle)$
    	\STATE receive $y_t$
    	\STATE $w_t \leftarrow w_{t-1} + \mathbbm{1}_{y_t \langle w_{t-1} , x_t \rangle \leq 0} \eta x_t$
    \ENDFOR
\end{algorithmic}
\end{algorithm}

As one can see, knowledge of $w_0$ and $\eta$ are critical to being able to teach an online perceptron learner. \cite{perceptron} looked at teaching a perceptron with and without knowledge of $w_0$. Here we look at the situation when the learning rate $\eta$ is unknown. As well we look at the most challenging case, when both $w_0$ and $\eta$ are unknown at the start of teaching. 


\section{Teaching Dimension}




\section*{References}



\small
\bibliography{paper_bib}
\bibliographystyle{plain}


\end{document}
