\documentclass{article}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{bbm}
\usepackage{parskip}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{color}

\setlength{\parskip}{5pt}
\setlength{\parindent}{0pt}

\title{\rule{350pt}{5pt}\\ \textbf{An Extension of Optimal Teaching For Online Perceptrons} \\ \rule{350pt}{1pt}}


\author{
  \textbf{Max Horowitz-Gelb \qquad Gabriel Ferns}\\
  Department of Computer Sciences, University of Wisconsin-Madison\\
  \texttt{\{horowitzgelb, ferns\}@wisc.edu}\\
}

\newcommand{\learn}{\mathcal{A}_{w_0}^\eta}


\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
  Optimal teaching is the task of constructing a special efficient training set, which when given to a learner teaches the desired concept inside the learners concept class. Before \cite{perceptron}, analysis of optimal teaching was only focused on batch learners. \cite{perceptron} looked at the optimal teaching strategy of an online Perceptron in two different scenarios with varying degrees of knowledge of the initial state of the learner. In this paper we show extensions of these scenarios where knowledge of the initial state is even more unknown.
\end{abstract}

\section{Introduction}
There may be a scenario where one entity, or the \textit{teacher},  has a desired target concept and a learner in which the teacher wants to embed that concept. In this scenario the teacher can only interact with the learner by constructing and then giving the learner a set of training examples. This constructive task is known as optimal teaching \cite{machine_teaching} \cite{teaching_dimension}. \cite{perceptron} looked for the first time at online teaching in an online environment, with an online perceptron. In this paper we look at optimal teaching of the same perceptron, but now we give the teacher the ability to use unlabeled examples. Unlabeled examples allow the teacher to probe to discover more about the learners current state while not changing the learner. We show that this new added ability allows the teacher to teach an online perceptron in a scenario, that before may have been difficult.

The specific perceptron we are teaching is the  online perceptron \cite{perceptron_algo}. The algorithm is designed for a binary classification. It can learn any linear decision boundary,
with mapping $x \to sign(\langle w, x \rangle)$ where $x \in \mathbb{R}^d$ comes from the input space and $w \in \mathbb{R}^d$ parameterizes the perceptron. Here , as in \cite{perceptron}, $sign(0) = -1$ for completeness. The learning algorithm for the online perceptron is seen below.

\begin{algorithm}
\caption{Online Perceptron}
\begin{algorithmic}[1]
	\REQUIRE Initial weight vector $w_0 \in \mathbb{R}^d$, learning rate $\eta > 0$
	\FOR{$t \in 1$ ...}
    	\STATE receive $x_t$
    	\STATE predict $\hat{y}_t = sign(\langle w_{t-1} , x_t \rangle)$
    	\STATE receive $y_t$
    	\STATE $w_t \leftarrow w_{t-1} + \mathbbm{1}_{y_t \langle w_{t-1} , x_t \rangle \leq 0} \eta x_t$
    \ENDFOR
\end{algorithmic}
\end{algorithm}

As one can see, knowledge of $w_0$ and $\eta$ are critical to being able to teach an online perceptron learner. \cite{perceptron} looked at teaching a perceptron with and without knowledge of $w_0$. Here we look at the situation when the learning rate $\eta$ is unknown. As well we look at the most challenging case, when both $w_0$ and $\eta$ are unknown at the start of teaching. 


\section{Teaching Dimension}
\cite{perceptron} used two metrics when analyzing their teaching strategies. The first was the exact 
teaching dimension. The exact teaching dimension is the length of the shortest training sequence that 
teaches a learner the desired concept. 

\subsection{$TD_{exact}$}

Formally, let $\mathbb{S}$ be the space of finite training 
sequences. Then let $\mathcal{A}_{w_0}^{\eta} : \mathbb{S} \to \mathbb{R}^d$ be a function the gives the 
resultant weight vector after training on a sequence in $\mathbb{S}$, where $w_0$ is the initial weight 
vector, and $\eta$ is the learning rate. Then if the teacher which to teach a concept $w^*$, then since $w^*$ describes a linear decision boundary then for any $c > 0$, $cw^*$ is parameterizes the same concept as $w^*$
so the exact teaching dimension is 
$$
TD_{exact}(w^*, \mathcal{A}_{w_0}^\eta) = \min\{ \lvert S \rvert : 
\mathcal{A}_{w_0}^\eta(S) = cw^* \text{ for some } c >0 \}
$$

\cite{perceptron} showed that when $\eta$ and $w_0$ where known, $TD_{exact}(w^*, \mathcal{A}_{w_0}^\eta) = 1$

\subsection{$TD_\epsilon$}
\cite{perceptron} also defined an approximate teaching dimension for cases where the exact teaching dimension was infinite but one could approximately teach a concept within $\epsilon$ distance. For perceptrons, the distance metric used was simply the 1 minus the cosine angle between $w^*$ 
and $\learn(S)$
$$
1 -\frac{\langle \learn(S), w^* \rangle}{\lVert \learn(s) \rVert \lVert w^* \rVert} \leq \epsilon
$$

Then let $\mathbb{T}_\epsilon$ be the set of $\epsilon-\text{approximate}$ teaching strategies for a decision boundary $w^*$. Then
$$
TD_\epsilon(w^*, \learn) = \min_{t \in 
\mathbb{T}_\epsilon} \max_{w_0 \in 
\mathbb{R}^d} \lvert t(w_0) \rvert
$$

\cite{perceptron} showed that when $w_0$ was unknown but $\eta$ was known $TD_\epsilon(w^*, \learn) = 3$

\section{Approximate Teaching with Unkown $\eta$}
Now we look at the scenario where the initial weight vector is known but the learning rate is unknown and bounded.

\subsection{Using Unlabeled Examples}
Before the teaching dimension was only analyzed with labeled examples. Here we relax this constraint and allow unlabeled examples to be used in the teaching sequence. Now we redefine $\mathbb{S}$ to be the set of all sequences of labeled and unlabeled examples. And for any $S \in \mathbb{S}$
$$
\learn(S) = \learn(S \setminus \{s \in S : s = (x, ?)\} )
$$

\subsection{Algorithm for bounding $\eta$}
With the ability to teach with unlabeled examples, below shows the optimal algorithm for bounding $\eta$ within a desired length, under the conditions that $\eta$ is bounded and $w_0$ is known a priori.

\begin{algorithm}
\caption{Bounding $\eta$ within $\epsilon$ }
\begin{algorithmic}[1]
	\REQUIRE $\epsilon$ , lower bound $\eta_{\min} > 0$, upper bound $\eta_{\max} < \infty$ and $w^*$
	\STATE Select $(x_1 , y_1)$ s.t. $sign(\langle w^* , x_1 \rangle) = y_1 = -1 = 
	-sign(\langle w_0 , x_1 \rangle )$
	\STATE Give $(x_1, y_1)$ to the perceptron and allow to update.
	\STATE $\alpha \leftarrow \eta_{\min}$
	\STATE $\beta \leftarrow \eta_{\max}$
	
	\WHILE{$\beta - \alpha > \epsilon$}
		\STATE Select $ x_2$ s.t. $\langle w_0 , x_2 \rangle \neq 0, \langle x_1 , x_2 \rangle \neq 0$ and $\langle w_0, x_2 \rangle - \frac{\beta - \alpha}{2}\langle x_1, x_2 \rangle = 0$
		\STATE Let the perceptron predict $\hat{y}$ for $x_2$
		\IF{$\hat{y} = 1$}
			\STATE $ \beta \leftarrow \frac{\beta - \alpha}{2}$ 		
		\ELSE
			\STATE $ \alpha \leftarrow \frac{\beta - \alpha}{2}$ 
		\ENDIF	
	\ENDWHILE   
	\RETURN $(\alpha , \beta)$
\end{algorithmic}
\end{algorithm} 

\textbf{Theorem 1.}
\textit{
If given a perceptron with known starting parameter $w_0$, desired parameter $w^*$, and unknown 
learning rate $\eta \in [\eta_{\min} , \eta_{\max}]$, then for given $\epsilon > 0$, \textbf{Algorithm 2} can guarantee $\eta \in 
[\alpha, \beta]$ such that $\beta - \alpha \leq \epsilon$ using $1 + \lceil \log_2(\frac{\eta_
{\min} - \eta_{\max}}{\epsilon})\rceil$ examples.}

\textit{Proof.}
Assume $w_0 \neq cw^*$ for arbitrary $c > 0$, then there exists an $(x_1, y_1)$ such that $\text{sign}(\langle w^*,x_1 \rangle) = y_1 = -1$ and $\text{sign}(\langle w_0,x_1 \rangle) = -y_1$.
\color{red}
I don't think that its hard to show this is true. But I won't prove for now
\color{black}

Then give $(x_1, y_1)$ to the learner and then after training update,
$$
w_1 = w_0 + \eta x_1
$$

Then grab an unlabeled $x_2$ such that
$\langle w_0,x_2 \rangle \neq 0$, $\langle x_1, x_2 \rangle \neq 0$
and 
$$
\langle w_0,x_2 \rangle - \frac{\eta_{\max} - \eta_{\min}}{2} \langle x_1, x_2 \rangle = 0
$$
\color{red}
There should always be at least one solution to this. Again I won't prove for now.
\color{black}


Then probe the learner with $x_2$. If the learner outputs $-1$ then
$\eta \in (\frac{\eta_{\max} - \eta_{\min}}{2} , \eta_{\max}]$, else 
$\eta \in [\eta_{\min},\frac{\eta_{\max} - \eta_{\min}}{2}]$

After the first labeled example , the length of the bound on $\eta$ is halved which each unlabeled example given. So one can see that with $n + 1$ examples the length of the bound on $\eta$ is $\frac{\eta_{\max} - \eta_{\min}}{2 ^ n }$. So to achieve a bound length of $\epsilon$ one needs
$ 1 + \lceil \log_2 ( \frac{\eta_
{\min} - \eta_{\max}}{\epsilon} ) \rceil$ examples.

\subsection{Teaching Dimension for Unknown $\eta$}

\textbf{Theorem 2.}
\textit{ If $w_0$ is known and $ 0 < \eta_{\min} \leq \eta \leq \eta_{\max} < \infty$ 
then $\mathbb{T}_{\epsilon}(w^*, \learn)  \leq 2 + \lceil \log_2 \frac{\eta_{\max} - \eta_{\min}}{2\epsilon\eta_{\max}}  \rceil$
}
\\
\\
\\
\textit{Proof.}
Assume WLOG $\lVert w^* \rVert = 1$.

Pick $(x_1,y_1)$ such that $y_1 = -1$, $\lVert x_1 \rVert = \frac{2 \lVert w_0 \rVert}{\epsilon \eta_{\min}}$ and $\langle x_1 , w^* \rangle = 0$.

Use this $(x_1, y_1)$ in \textbf{Algorithm 2} to get a bound on $\eta$ such that $\eta \in [\alpha , \beta]$ and $\beta - \alpha <= 2\epsilon \eta_{\max}$

Then let $x_2 = \frac{2 \lVert w_0 \rVert}{\epsilon (\beta + \alpha)/2}w^* + x_1$.
It should be clear that then that $sign(\langle w* , x_2 \rangle) =  y_2 = 1$

Then realize that
$w_1 = w_0 - \eta x_1$

So
$$
y_2 \langle w_1, x_2 \rangle  = \bigg\langle \frac{2 \lVert w_0 \rVert}{\epsilon (\beta + \alpha) /2} w^* + x_1 , w_0 - \eta x_1 \bigg\rangle
$$
$$
= \bigg\langle \frac{2 \lVert w_0 \rVert}{\epsilon (\beta + \alpha) /2} w^* + x_1 , w_0  \bigg\rangle - \eta \lVert x_1 \rVert^2
$$
$$
\leq \bigg( \frac{2 \lVert w_0 \rVert}{\epsilon (\beta + \alpha) /2} \lVert w^* \rVert + \lVert x_1 \rVert \bigg ) \lVert w_0 \rVert - \eta \lVert x_1 \rVert^2
$$
$$
= \frac{2 \lVert w_0 \rVert^2}{\epsilon (\beta + \alpha)/2} + \frac{2 \lVert w_0 \rVert ^2}{\epsilon \eta_{\min}} - \eta \frac{4 \lVert w_0 \rVert^2}{\epsilon^2 \eta_{\min}^2}
$$
$$
\leq \frac{4 \lVert w_0 \rVert^2}{\epsilon (\beta + \alpha)/2}  -  \frac{4 \lVert w_0 \rVert^2}{\epsilon^2 \eta_{\min}}
$$
which is less than $0$ for $\epsilon < 1$, so $(x_2 , y_2)$ will cause the perceptron to update.

Then,
notice that $w_2 = w_0 +\eta \frac{2  \lVert w_0 \rVert}{\epsilon (\beta + \alpha)/2} w^*$

Then 
$$
\frac{\langle w_2, w^* \rangle}{\lVert w_1 \rVert \lVert w^* \rVert} = \frac{
\frac{\eta 2  \lVert w_0 \rVert}{\epsilon (\beta + \alpha)/2} \lVert w^* \lVert ^2 + \langle w_0, w^* \rangle}{\lVert w_2 \rVert}
$$ 
$$
\geq \frac{\frac{\eta 2  \lVert w_0 \rVert}{\epsilon (\beta + \alpha)/2} - \lVert w_0 \rVert}{\lVert w_0 \rVert +\frac{\eta 2  \lVert w_0 \rVert}{\epsilon (\beta + \alpha)/2} }
$$
$$
= 1 - \frac{2 \lVert w_0  \rVert}{\lVert w_0 \rVert +\frac{\eta 2  \lVert w_0 \rVert}{\epsilon (\beta + \alpha)/2} }
$$
$$
= 1 - \frac{2\lVert w_0 \rVert}{\lVert w_0  \rVert \epsilon + \frac{\eta 2  \lVert w_0 \rVert}{(\beta + \alpha)/2} } \epsilon
$$
$$
\geq  1 - \frac{2\lVert w_0 \rVert}{\lVert w_0  \rVert \epsilon + \frac{\eta 2  \lVert w_0 \rVert}{\eta_{\max} - (\beta - \alpha) / 2} } \epsilon
$$
$$
\geq  1 - \frac{2\lVert w_0 \rVert}{\lVert w_0  \rVert \epsilon + \frac{\eta 2  \lVert w_0 \rVert}{\eta_{\max} (1-\epsilon)} } \epsilon
$$
$$
\geq 1 - \frac{2\lVert w_0 \rVert}{\lVert w_0  \rVert \epsilon + \frac{ 2  \lVert w_0 \rVert}{(1-\epsilon)} } \epsilon
$$
$$
\geq 1 - \epsilon
$$

Therefore 
$T_\epsilon \leq  2 + \lceil \log_2 \frac{\eta_{\max} - \eta_{\min}}{2\epsilon\eta_{\max}}  \rceil$


\section*{References}



\small
\bibliography{paper_bib}
\bibliographystyle{plain}


\end{document}
